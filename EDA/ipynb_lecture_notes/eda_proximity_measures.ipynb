{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximity Measures: Similarity, Dissimilarity, Distance\n",
    "\n",
    "* __Author: Prof. Nagiza F. Samatova__\n",
    "* __Email: samatova@csc.ncsu.edu__\n",
    "* __Date: October 3, 2018__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "# http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.pairwise\n",
    "\n",
    "from scipy.stats import pearsonr as _pearson_cor\n",
    "# https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.pearsonr.html\n",
    "\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.distance_metrics.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we will cover ___proximity measures___, including:\n",
    "+ _similarity_ measures\n",
    "+ _dissimilarity_ measures, and \n",
    "+ _distance_ metrics. \n",
    "\n",
    "Measuring the proximity between two objects is an important step in many machine learning and data mining tasks including but not limited to:\n",
    "+ clustering\n",
    "+ nearest neighbor classification\n",
    "+ missing links prediction\n",
    "+ community detection in social networks\n",
    "+ recommendation systems such as collaborative filtering\n",
    "\n",
    "> The proximity between two objects is a numerical measure of the degree to which the two objects are alike. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity vs. Dissimilarity vs. Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proximity often refers to a similarity or dissimilarity:\n",
    "+ __Similarity__:\n",
    "    - Numerical measure of how alike two data objects are.\n",
    "    - Is higher when objects are more alike.\n",
    "    - Often falls in the range [0,1]:\n",
    "    - Examples: Cosine, Jaccard, Tanimoto,\n",
    "+ __Dissimilarity__:\n",
    "    - Numerical measure of how different two data objects are\n",
    "    - Lower when objects are more alike\n",
    "    - Minimum dissimilarity is often 0\n",
    "    - Upper limit varies\n",
    "+ __Distance__:\n",
    "    - A special class of dissimilarities with some particular mathematical properties\n",
    "    - Ranges from 0 to positive infinity\n",
    "    - Value of 0 means no differences between two objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The _similarity_ between two objects is a numerical measure of the degree to which the two objects are alike. Similarities are higher for pairs of objects that are more alike, and often range from 0 (no similarity) to 1 (complete similarity).\n",
    "+ The _dissimilarity_ between two objects is a numerical measure of the degree to which the two objects are different. Dissimilarities are lower for more similar pairs of objects, and sometimes range from 0 (no dissimilarity) to 1 (complete dissimilarity)\n",
    "+ Similarities can often be transformed to dissimilarities in a straightforward way:\n",
    "    - If the similarity ($s$) falls in the interval $[0,1]$, then the dissimilarity ($d$) can be defined as:\n",
    "    > $d = 1 - s$\n",
    "+ _Distance_ is a special class of dissimilarities with some particular mathematical properties. Distances usually range from 0 to infinity.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity: Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $s(x,y)$ is the _similarity_ between points $x$ and $y$, \n",
    "then the typical properties of similarities are the following: \n",
    "+ $s(x,y) = 1$ only if $x = y$ \n",
    "+ $0 <= s <= 1$, and \n",
    "+ $s(x,y) = s(y,x)$ for all $x$ and $y$ (_Symmetry_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity for Binary (0, 1) Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity measures for objects with only _binary_ attributes (e.g., binary vectors): \n",
    "+ _SMC_: Simple Matching Coefficient\n",
    "+ _Jaccard_ coefficient:\n",
    "    - Ignores zero matches\n",
    "    - Desirable when two records should not be similar because a large number of characteristics are absent in both:\n",
    "        * Document-document similarity: matching Words used\n",
    "        * User-User similarity: matching Items purchased\n",
    "+ _Tanimoto_ coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "First, define two binary vectors: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 1 0 1 0 0 0]\n",
      "[0 0 1 0 0 1 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = np.array((1,0,0,0,1,0,1,0,0,0))\n",
    "v2 = np.array ((0,0,1,0,0,1,0,0,0,0))\n",
    "print (v1)\n",
    "print (v2)\n",
    "sum (v1 == v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ___simple matching coefficient___ (__SMC__) for two binary vectors is given by the number of attributes (i.e., vector components) where both vectors have the same value (i.e., both have a value of 0 or both have a value of 1) over the total number of attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smc (b1, b2):\n",
    "    return sum( b1 == b2) / b1.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smc (v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ___Jaccard___ coefficient for two binary vectors is given by the number of attributes (i.e., vector components) where both vectors have a value of 1 over the number of attributes where at least one of the vectors has a value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard (b1, b2):\n",
    "    return sum((b1==1)&(b2==1))/sum((b1==1)|(b2==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard (v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the similarities obtained for $v1$ and $v2$ using the SMC and the Jaccard coefficient are very different:\n",
    "+ This is because the SMC measures similarity in terms of __both__ matching 0's and matching 1's, while the Jaccard coefficient only considers matching 1s. \n",
    "+ As a result, the Jaccard coefficient is frequently used to handle objects consisting of _asymmetric_ binary distributions:\n",
    "    - for example, customer transactions at a store, where the matching 0's (i.e., store items not bought in neither of the transactions) are likely to greatly outnumber the matching 1's (i.e., store items bought in both transactions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the Jaccard coefficient ranges between 0 and 1, then the dissimilarity between $v1$ and $v2$ based on their Jaccard coefficient can be defined as:\n",
    "> $1 - $ jaccard $(v1, v2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1- jaccard (v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity for Non-Binary Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity measures for objects with _non-binary_ (i.e., discrete or continuous) attributes: \n",
    "+ _Cosine_ similarity\n",
    "+ _Pearson_ correlation\n",
    "+ _Spearman_ correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Cosine___ similarity is a measure of similarity between two vectors given by the cosine of the angle between them:\n",
    "+ The cosine similarity between two vectors can be computed by dividing their scalar product by the product of their norms:\n",
    "> $cos (x, y) = \\dfrac{(x, y)} { ||x|| \\times ||y||}$\n",
    "+ Note that cosine similarity _compares two vectors only in terms of their directions_, and not their norms (i.e., magnitudes). \n",
    "+ Also note that cosine similarity ranges from -1 to 1:\n",
    "    + __1__ if the two vectors have the same direction, \n",
    "    + __-1__ if they have opposite directions, and \n",
    "    + __0__ if they are orthogonal (i.e., perpendicular) to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity is commonly used in information retrieval and text mining to compute the similarity between two documents:\n",
    "+ Documents are often represented as vectors, where each attribute represents the frequency with which a particular term (word) occurs in the document.\n",
    "+ These documents are generally very sparse (i.e., have relatively few non-zero attributes out of thousands or tens of thousands of attributes). \n",
    "+ Therefore, as with customer transactions, similarity between documents must not consider matching 0's; otherwise, most documents would be highly similar. \n",
    "+ The use of cosine similarity is appropriate in this case because:\n",
    "    - like the Jaccard coefficient, it does not consider matching 0's when computing the similarity between two objects, but, \n",
    "    - unlike the Jaccard coefficient, it can also handle non-binary attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cos_similarity (x, y):\n",
    "    \"\"\"Takes 2 vectors x, y and returns the cosine similarity according \n",
    "    to the definition of the dot product\n",
    "    \"\"\"\n",
    "    dot_product = np.inner(x, y)\n",
    "    norm_x = np.linalg.norm(x)\n",
    "    norm_y = np.linalg.norm(y)\n",
    "    return dot_product / (norm_x * norm_y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 0, 0, 5, 0, 2, 0, 0, 0]\n",
      "[1, 0, 3, 0, 2, 1, 3, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "v3 = [2,1,0,0,5,0,2,0,0,0]\n",
    "v4 = [1,0,3,0,2,1,3,0,0,0]\n",
    "print (v3)\n",
    "print (v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6301260378126045"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity(v3,v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63012604]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html\n",
    "\n",
    "# note that 1D arrays is not passable\n",
    "cosine_similarity([v3], [v4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that passing one dimension arrays as input data is deprecated in sklearn version 0.17, and will raise ValueError:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to pass one-dimensional array : Expected 2D array, got 1D array instead:\n",
      "array=[2. 1. 0. 0. 5. 0. 2. 0. 0. 0.].\n",
      "Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Should generate ValueError\n",
    "try:\n",
    "    cosine_similarity(v3, v4)\n",
    "except ValueError as e:\n",
    "    print ('Unable to pass one-dimensional array : {} \\n'.format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6301260378126045"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another option:\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "result = 1 - spatial.distance.cosine(v3, v4)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://stackoverflow.com/questions/18424228/cosine-similarity-between-2-number-lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson and Spearman Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Pearson___ correlation measures the _linear_ relationship between objects:\n",
    "\n",
    "+ Strictly speaking, Pearson’s correlation requires that each dataset be normally distributed. \n",
    "+ It ranges between -1 and +1 with:\n",
    "    - 0 implying no correlation. \n",
    "    - Correlations of -1 or +1 imply an exact linear relationship. \n",
    "+ Positive correlations imply that as x increases, so does y. \n",
    "+ Negative correlations imply that as x increases, y decreases.\n",
    "+ The $p$-value of the correlation roughly indicates the probability of an uncorrelated system producing datasets that have a Pearson correlation at least as extreme as the one computed from these datasets. \n",
    "    - The $p$-values are not entirely reliable but are probably reasonable for datasets larger than 500 or so.\n",
    "\n",
    "To compute correlation, we standardize data objects, $p$ and $q$ to their $Z$-scores by subtracting the mean and dividing by standard diviation, and then take the dot product of their $Z$-scores:\n",
    "```python\n",
    "pZ = (p - mean(p))/sd(p)\n",
    "qZ = (q - mean(q))/sd(q)\n",
    "cor(p, q) = (pZ, qZ)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [15, 12, 8, 8, 7, 7, 7, 6, 5, 3]\n",
    "b = [10, 25, 17, 11, 13, 17, 20, 13, 9, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option-0:\n",
    "\n",
    "def corr(u, v):\n",
    "    \"u & v should be numpy arrays.\"\n",
    "    u = np.array(u)\n",
    "    v = np.array(v)\n",
    "    mean1 = u.mean() \n",
    "    mean2 = v.mean()\n",
    "    std1 = u.std()\n",
    "    std2 = v.std()\n",
    "\n",
    "#     corr = ((data1-mean1)*(data2-mean2)).mean()/(std1*std2)\n",
    "    corr = ((u*v).mean()-mean1*mean2)/(std1*std2)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14499815458068538"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr (a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option-1\n",
    "import math\n",
    "\n",
    "def average(x):\n",
    "    assert len(x) > 0\n",
    "    return float(sum(x)) / len(x)\n",
    "\n",
    "def pearson_cor(x, y):\n",
    "    assert len(x) == len(y)\n",
    "    n = len(x)\n",
    "    assert n > 0\n",
    "    avg_x = average(x)\n",
    "    avg_y = average(y)\n",
    "    diffprod = 0\n",
    "    xdiff2 = 0\n",
    "    ydiff2 = 0\n",
    "    for idx in range(n):\n",
    "        xdiff = x[idx] - avg_x\n",
    "        ydiff = y[idx] - avg_y\n",
    "        diffprod += xdiff * ydiff\n",
    "        xdiff2 += xdiff * xdiff\n",
    "        ydiff2 += ydiff * ydiff\n",
    "\n",
    "    return diffprod / math.sqrt(xdiff2 * ydiff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14499815458068516"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_cor (a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14499815458068518\n",
      "0.6894014481166955\n"
     ]
    }
   ],
   "source": [
    "# Option-2:\n",
    "from scipy.stats import pearsonr as _pearson_cor\n",
    "\n",
    "cor, pval = _pearson_cor(a, b)\n",
    "print (cor)\n",
    "print (pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinregressResult(slope=0.20833333333333331, intercept=13.375, rvalue=0.14499815458068518, pvalue=0.689401448116695, stderr=0.5026170462708364)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option-3:\n",
    "from scipy.stats import linregress\n",
    "\n",
    "linregress(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "numpy.corrcoef?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14499815458068518"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option-4:\n",
    "import numpy\n",
    "\n",
    "numpy.corrcoef(a, b)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15, 10), (12, 25), (8, 17), (8, 11), (7, 13), (7, 17), (7, 20), (6, 13), (5, 9), (3, 15)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.144998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.144998</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  1.000000  0.144998\n",
       "1  0.144998  1.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option-5:\n",
    "import pandas as pd\n",
    "\n",
    "a = [15, 12, 8, 8, 7, 7, 7, 6, 5, 3]\n",
    "b = [10, 25, 17, 11, 13, 17, 20, 13, 9, 15]\n",
    "\n",
    "data = list (zip(a, b))\n",
    "print (data)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Distance___ is a special class of dissimilarities with some particular mathematical properties. Distances usually range from 0 (identity between two objects) to positive infinity.\n",
    "\n",
    "> If $d(x,y)$ is the distance between points $x$ and $y$, then the following properties hold: \n",
    "+ $d(x,y) >= 0$ for all $x$ and $y$, \n",
    "+ $d(x,y) = 0$ only if $x = y$ (Positivity),\n",
    "+ $d(x,y) = d(y,x)$ for all $x$ and $y$ (Symmetry), and \n",
    "+ $d(x,z) <= d(x,y) + d(y,z)$ for all $x$, $y$, and $z$ (Triangle Inequality)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Distance measures for objects with continuous attributes include:\n",
    "    - _Euclidean_ distance:\n",
    "        * Attributes must be standarded to $Z$-scores if their scales differ!\n",
    "    - _Minkowski_ distance:\n",
    "        * Manhattan (city block) distance\n",
    "    - _Mahalanobis_ distance:\n",
    "        * Makes attributes that are highly correlated with other attributes not to contribute as much to the distance\n",
    "+ Distance measures for objects with categorical attributes include:\n",
    "    - _Hamming_ distance:\n",
    "        * The distance is 0 if the attributes are in the same category and 1, otherwise\n",
    "        * Measures the number of bits that are different between two binary vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance\n",
    "\n",
    "The most commonly used distance metric is the Euclidean distance. The Euclidean distance between two vectors (in one-, two-, three-, or any higher “n”-dimensional space) is given by the square root of the sum of the squared differences between the corresponding vector components:\n",
    "> $dist (x, y) = \\sqrt {\\sum (x_k - y_k)^2}$\n",
    "\n",
    "+ https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy\n",
    "+ http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.196152422706632"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option-1\n",
    "\n",
    "def eucl_dist(x,y):   \n",
    "    return np.sqrt(np.sum((x-y)**2))\n",
    "\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "dist = eucl_dist(a,b)\n",
    "\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.196152422706632"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option-2\n",
    "from  numpy.linalg import norm\n",
    "\n",
    "a = (1, 2, 3)\n",
    "b = (4, 5, 6)\n",
    "dist = np.linalg.norm(np.array (a) - np.array(b))\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.196152422706632"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option-3:\n",
    "from scipy.spatial import distance\n",
    "a = (1, 2, 3)\n",
    "b = (4, 5, 6)\n",
    "dist = distance.euclidean(a, b)\n",
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Finally, always keep in mind that the proximity measure you use “should fit the type of data. For many types of dense, continuous data, metric distance measures such as Euclidean distance are often used. For sparse data, which often consists of asymmetric attributes, we typically employ similarity measures that ignore 0-0 matches, such as the Jaccard coefficient or cosine similarity. Conceptually, this reflects the fact that, for a pair of complex objects, similarity depends on the number of characteristics they both share, rather than the number of characteristics they both lack."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
